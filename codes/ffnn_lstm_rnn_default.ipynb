{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "XgaUxenqwofQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "DXSp2E2ZTm5C"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ]
    },
    {
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "# Import nltk before using it\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Step 1: Load your datasets\n",
        "train_data = pd.read_csv('incidents_labelled.csv')  # Training dataset with labels\n",
        "test_data = pd.read_csv('incidents_val.csv')  # Test dataset without labels"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ooYDs7Ot6mF",
        "outputId": "96a625ef-b94b-4e16-a669-906f97694aa5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All Classes"
      ],
      "metadata": {
        "id": "gakqzD8owrgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocessing Function - Text Cleaning\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Apply text preprocessing\n",
        "train_data['cleaned_text'] = train_data['text'].apply(preprocess_text)\n",
        "test_data['cleaned_text'] = test_data['text'].apply(preprocess_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cdg9bwW6feOa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Label Binarization (MultiLabelBinarizer for multi-label classification)\n",
        "labels = ['hazard-category', 'product-category', 'hazard', 'product']\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Apply MultiLabelBinarizer to the labels in the training data\n",
        "y_train = mlb.fit_transform(train_data[labels].values)\n",
        "\n",
        "# Step 4: Split the data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_data['cleaned_text'], y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Vectorize text using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features if needed\n",
        "\n",
        "# Fit TF-IDF on training data and transform both training and validation sets\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n"
      ],
      "metadata": {
        "id": "VP2Vf4HefeL7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Create PyTorch Dataset Class for Multi-Label Data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X.toarray(), dtype=torch.float32)  # Convert sparse matrix to dense array\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)  # Ensure labels are float\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create PyTorch Dataset and DataLoader for training and validation\n",
        "train_dataset = TextDataset(X_train_tfidf, y_train)\n",
        "val_dataset = TextDataset(X_val_tfidf, y_val)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "m1LcfoiSfeGb"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define FFNN Model\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Step 8: Define LSTM Model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(1, x.size(0), 128).to(x.device)\n",
        "        c_0 = torch.zeros(1, x.size(0), 128).to(x.device)\n",
        "        out, _ = self.lstm(x.unsqueeze(1), (h_0, c_0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Step 9: Define RNN Model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(1, x.size(0), 128).to(x.device)\n",
        "        out, _ = self.rnn(x.unsqueeze(1), h_0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return self.sigmoid(out)\n"
      ],
      "metadata": {
        "id": "ip5kXrBWvhEp"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Initialize models\n",
        "input_size = X_train_tfidf.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "model_ffnn = FFNN(input_size, output_size).to('cpu')\n",
        "model_lstm = LSTM(input_size, 128, output_size).to('cpu')\n",
        "model_rnn = RNN(input_size, 128, output_size).to('cpu')\n",
        "\n",
        "# Step 11: Training Setup\n",
        "optimizer_ffnn = optim.Adam(model_ffnn.parameters(), lr=0.001)\n",
        "optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
        "optimizer_rnn = optim.Adam(model_rnn.parameters(), lr=0.001)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "GzfcezoWvlBR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Training Loop with Accuracy\n",
        "def train_model(model, optimizer):\n",
        "    model.train()\n",
        "    for epoch in range(5):  # Train for 5 epochs\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for batch_X, batch_y in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_X)\n",
        "            loss = loss_fn(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = (output > 0.5).float()  # Convert to binary predictions\n",
        "            correct_predictions += (preds == batch_y).sum().item()\n",
        "            total_predictions += np.prod(batch_y.size())\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        epoch_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "bZJC0K3vvk-Z"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 13: Combined Evaluation for All Labels\n",
        "def evaluate_model_combined(model, label_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Disable gradient calculation for evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_dataloader:\n",
        "            output = model(batch_X)\n",
        "            preds = (output > 0.5).cpu().numpy()  # Binary classification (multi-label)\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(batch_y.cpu().numpy())\n",
        "\n",
        "    # Stack all predictions and labels for the evaluation\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "\n",
        "    # Evaluate each label separately\n",
        "    for i, label_name in enumerate(label_names):\n",
        "        print(f\"Classification report for {label_name}:\")\n",
        "        print(classification_report(all_labels[:, i], all_preds[:, i], target_names=[f\"{label_name} class 0\", f\"{label_name} class 1\"]))\n",
        "\n",
        "    # Combined classification report for all labels\n",
        "    print(\"Combined classification report for all labels:\")\n",
        "    print(classification_report(all_labels, all_preds))\n",
        "\n",
        "    # Compute overall accuracy for multi-label classification\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Overall accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Example labels (should match your dataset's labels)\n",
        "label_names = ['hazard-category', 'product-category', 'hazard', 'product']\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3n_NrlQ7v-1M"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Generate Predictions on Test Set\n",
        "def predict_on_test(model):\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch_X in DataLoader(torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32), batch_size=32):\n",
        "            output = model(batch_X)\n",
        "            preds = (output > 0.5).cpu().numpy()\n",
        "            test_predictions.append(preds)\n",
        "\n",
        "    test_predictions = np.vstack(test_predictions)\n",
        "    predicted_labels = mlb.inverse_transform(test_predictions)\n",
        "    test_data['predicted_labels'] = predicted_labels\n",
        "    test_data.to_csv('test_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "id": "u1U8L2ARwD90"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FFNN"
      ],
      "metadata": {
        "id": "0uaQls7ovZJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train FFNN\n",
        "print(\"Training FFNN:\")\n",
        "train_model(model_ffnn, optimizer_ffnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0IDu6luvc1o",
        "outputId": "ac3f1c8b-5850-49c2-9504-a06cfa583dd2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FFNN:\n",
            "Epoch 1, Loss: 0.18681446527441342, Accuracy: 96.66%\n",
            "Epoch 2, Loss: 0.0154550690886875, Accuracy: 99.70%\n",
            "Epoch 3, Loss: 0.014023678017159303, Accuracy: 99.70%\n",
            "Epoch 4, Loss: 0.013442544527351856, Accuracy: 99.71%\n",
            "Epoch 5, Loss: 0.01276042057822148, Accuracy: 99.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate FFNN\n",
        "print(\"Evaluating FFNN:\")\n",
        "evaluate_model(model_ffnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_dWbQn-v8l8",
        "outputId": "b4207afb-851e-4319-e12e-ddebac571d54"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating FFNN:\n",
            "Classification report for hazard-category:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1194\n",
            "         1.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       0.99      1.00      1.00      1197\n",
            "\n",
            "Classification report for product-category:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1196\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       1.00      1.00      1.00      1197\n",
            "\n",
            "Classification report for hazard:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1197\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       1.00      1.00      1.00      1197\n",
            "weighted avg       1.00      1.00      1.00      1197\n",
            "\n",
            "Classification report for product:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      1.00      1189\n",
            "         1.0       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.99      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       0.99      0.99      0.99      1197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "D2BSSQkCwMRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LSTM\n",
        "print(\"Training LSTM:\")\n",
        "train_model(model_lstm, optimizer_lstm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGRa6IfHv8jc",
        "outputId": "a594a6f3-b830-4e73-e51b-d078a0fc4b35"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM:\n",
            "Epoch 1, Loss: 0.23747163527955611, Accuracy: 96.46%\n",
            "Epoch 2, Loss: 0.018571319822221995, Accuracy: 99.70%\n",
            "Epoch 3, Loss: 0.014982426458348831, Accuracy: 99.70%\n",
            "Epoch 4, Loss: 0.014161314809074005, Accuracy: 99.70%\n",
            "Epoch 5, Loss: 0.013833714922269185, Accuracy: 99.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate LSTM\n",
        "print(\"Evaluating LSTM:\")\n",
        "evaluate_model(model_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WKIUQOIwWoe",
        "outputId": "58d6a874-b85e-4c86-ffe6-c762df2e4f3f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating LSTM:\n",
            "Classification report for hazard-category:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1194\n",
            "         1.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       0.99      1.00      1.00      1197\n",
            "\n",
            "Classification report for product-category:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1196\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       1.00      1.00      1.00      1197\n",
            "\n",
            "Classification report for hazard:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1197\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       1.00      1.00      1.00      1197\n",
            "weighted avg       1.00      1.00      1.00      1197\n",
            "\n",
            "Classification report for product:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      1.00      1189\n",
            "         1.0       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.99      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       0.99      0.99      0.99      1197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "lhHjV1VLwOM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train RNN\n",
        "print(\"Training RNN:\")\n",
        "train_model(model_rnn, optimizer_rnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuYZweuNv8g0",
        "outputId": "5a0a7161-2708-4d66-af4d-6dcf6353e682"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN:\n",
            "Epoch 1, Loss: 0.15474116284400224, Accuracy: 97.86%\n",
            "Epoch 2, Loss: 0.01549387101083994, Accuracy: 99.70%\n",
            "Epoch 3, Loss: 0.01398799628019333, Accuracy: 99.70%\n",
            "Epoch 4, Loss: 0.013597080080459515, Accuracy: 99.70%\n",
            "Epoch 5, Loss: 0.013418644741177559, Accuracy: 99.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate RNN\n",
        "print(\"Evaluating RNN:\")\n",
        "evaluate_model(model_rnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp_SPoYdv8ec",
        "outputId": "620b9155-b651-43bd-ab75-93af825e0b61"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RNN:\n",
            "Classification report for hazard-category:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1194\n",
            "         1.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       0.99      1.00      1.00      1197\n",
            "\n",
            "Classification report for product-category:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1196\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       1.00      1.00      1.00      1197\n",
            "\n",
            "Classification report for hazard:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1197\n",
            "\n",
            "    accuracy                           1.00      1197\n",
            "   macro avg       1.00      1.00      1.00      1197\n",
            "weighted avg       1.00      1.00      1.00      1197\n",
            "\n",
            "Classification report for product:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      1.00      1189\n",
            "         1.0       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.99      1197\n",
            "   macro avg       0.50      0.50      0.50      1197\n",
            "weighted avg       0.99      0.99      0.99      1197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict on Test Set"
      ],
      "metadata": {
        "id": "e7Qimtl3wbmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "#print(\"Predicting with FFNN:\")\n",
        "#predict_on_test(model_ffnn)"
      ],
      "metadata": {
        "id": "YieZLDWwv8bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9q3Bb1XUwj4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
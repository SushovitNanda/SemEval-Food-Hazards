{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfggcYp59P8NUZ0ok/rnho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushovitNanda/SemEval-Food-Hazards/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Rdk7SjPnjA2P"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('incidents_labelled.csv')\n",
        "\n",
        "# Combine 'title' and 'text' columns for richer input data\n",
        "data['combined_text'] = data['title'] + \" \" + data['text']\n",
        "\n",
        "# Encode hazard-category to numerical labels\n",
        "label_mapping = {label: idx for idx, label in enumerate(data['hazard-category'].unique())}\n",
        "data['label'] = data['hazard-category'].map(label_mapping)\n",
        "\n",
        "# Stratified train-test split\n",
        "train_df, val_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
        "\n",
        "# Define tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping))\n"
      ],
      "metadata": {
        "id": "jdHH1x_ps9Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class for PyTorch\n",
        "class HazardDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = HazardDataset(train_df['combined_text'].tolist(), train_df['label'].tolist(), tokenizer)\n",
        "val_dataset = HazardDataset(val_df['combined_text'].tolist(), val_df['label'].tolist(), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "AFc_gd23tAE9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        for batch in tqdm(train_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation after each epoch\n",
        "        evaluate_model(model, val_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"Validation Classification Report:\")\n",
        "    print(classification_report(true_labels, predictions, target_names=label_mapping.keys()))\n",
        ""
      ],
      "metadata": {
        "id": "GScmMq48tMVN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, val_loader, optimizer, scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCtRQP3ktXM9",
        "outputId": "301cc95d-bf57-4216-bde9-a26b2189c98f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:12<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Training Loss: 0.7295878298332293\n",
            "Validation Classification Report:\n",
            "                                precision    recall  f1-score   support\n",
            "\n",
            "                    biological       0.87      0.94      0.90       404\n",
            "                foreign bodies       0.90      0.84      0.87       154\n",
            "                      chemical       0.82      0.88      0.85       100\n",
            "                         fraud       0.65      0.55      0.60        82\n",
            "          organoleptic aspects       1.00      0.23      0.38        13\n",
            "                     allergens       0.91      0.96      0.94       391\n",
            "              packaging defect       1.00      0.06      0.12        16\n",
            "                  other hazard       0.33      0.31      0.32        29\n",
            "food additives and flavourings       0.00      0.00      0.00         5\n",
            "                     migration       0.00      0.00      0.00         3\n",
            "\n",
            "                      accuracy                           0.86      1197\n",
            "                     macro avg       0.65      0.48      0.50      1197\n",
            "                  weighted avg       0.85      0.86      0.85      1197\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:14<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 - Training Loss: 0.3201268105643491\n",
            "Validation Classification Report:\n",
            "                                precision    recall  f1-score   support\n",
            "\n",
            "                    biological       0.95      0.95      0.95       404\n",
            "                foreign bodies       0.83      0.94      0.88       154\n",
            "                      chemical       0.84      0.91      0.88       100\n",
            "                         fraud       0.85      0.65      0.74        82\n",
            "          organoleptic aspects       1.00      0.54      0.70        13\n",
            "                     allergens       0.92      0.96      0.94       391\n",
            "              packaging defect       0.75      0.38      0.50        16\n",
            "                  other hazard       0.76      0.66      0.70        29\n",
            "food additives and flavourings       0.00      0.00      0.00         5\n",
            "                     migration       0.00      0.00      0.00         3\n",
            "\n",
            "                      accuracy                           0.90      1197\n",
            "                     macro avg       0.69      0.60      0.63      1197\n",
            "                  weighted avg       0.90      0.90      0.90      1197\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:15<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 - Training Loss: 0.18963559216819703\n",
            "Validation Classification Report:\n",
            "                                precision    recall  f1-score   support\n",
            "\n",
            "                    biological       0.93      0.96      0.95       404\n",
            "                foreign bodies       0.90      0.90      0.90       154\n",
            "                      chemical       0.84      0.90      0.87       100\n",
            "                         fraud       0.83      0.66      0.73        82\n",
            "          organoleptic aspects       0.88      0.54      0.67        13\n",
            "                     allergens       0.92      0.96      0.94       391\n",
            "              packaging defect       0.88      0.44      0.58        16\n",
            "                  other hazard       0.63      0.66      0.64        29\n",
            "food additives and flavourings       1.00      0.20      0.33         5\n",
            "                     migration       0.00      0.00      0.00         3\n",
            "\n",
            "                      accuracy                           0.90      1197\n",
            "                     macro avg       0.78      0.62      0.66      1197\n",
            "                  weighted avg       0.90      0.90      0.90      1197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1t8GwIEtthjx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
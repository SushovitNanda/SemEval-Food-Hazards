{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushovitNanda/SemEval-Food-Hazards/blob/main/HC_Mix_85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d_6T0SB66yLg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers datasets scikit-learn torch\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kM9m7aEl6h_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b634b8cf-28cb-4bf5-8d71-e87bb8bc9c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from nltk.corpus import wordnet\n",
        "import gensim\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Disable W&B logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Rc9voMsG668s"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Load Dataset\n",
        "# -------------------------------\n",
        "train = pd.read_csv('incidents_train.csv')\n",
        "\n",
        "# Combine title and text columns\n",
        "train['combined_text'] = train['title'] + ' ' + train['text']\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train['hazard_category_encoded'] = label_encoder.fit_transform(train['hazard-category'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train['combined_text'], train['hazard_category_encoded'], test_size=0.2, random_state=42, stratify=train['hazard_category_encoded']\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Data Augmentation: Synonym Replacement\n",
        "# -------------------------------\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        word_idx = random.randint(0, len(words) - 1)\n",
        "        synonyms = wordnet.synsets(words[word_idx])\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words[word_idx] = synonym\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "X_train_augmented = X_train.apply(lambda x: synonym_replacement(x))\n",
        "\n",
        "# Combine original and augmented data\n",
        "X_train_combined = pd.concat([X_train, X_train_augmented])\n",
        "y_train_combined = pd.concat([y_train, y_train])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Improved Preprocessing\n",
        "# -------------------------------\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "X_train_preprocessed = X_train_combined.apply(preprocess_text)\n",
        "X_val_preprocessed = X_val.apply(preprocess_text)\n",
        "\n",
        "# -------------------------------\n",
        "# Word2Vec Embeddings\n",
        "# -------------------------------\n",
        "X_train_tokenized = X_train_preprocessed.apply(lambda x: gensim.utils.simple_preprocess(x))\n",
        "X_val_tokenized = X_val_preprocessed.apply(lambda x: gensim.utils.simple_preprocess(x))\n",
        "\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=X_train_tokenized, vector_size=100, window=5, min_count=2, workers=4)\n",
        "word2vec_model.train(X_train_tokenized, total_examples=len(X_train_tokenized), epochs=10)\n",
        "\n",
        "def get_average_word2vec(tokens, model, vector_size=100):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "X_train_word2vec = np.array([get_average_word2vec(tokens, word2vec_model) for tokens in X_train_tokenized])\n",
        "X_val_word2vec = np.array([get_average_word2vec(tokens, word2vec_model) for tokens in X_val_tokenized])\n",
        "\n",
        "# -------------------------------\n",
        "# TF-IDF Features\n",
        "# -------------------------------\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_preprocessed).toarray()\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val_preprocessed).toarray()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PzZh3vMboOV",
        "outputId": "aef8beaa-2344-4242-a988-d97da1bd4cfe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# DistilBERT Embeddings\n",
        "# -------------------------------\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "train_encodings = tokenizer(X_train_preprocessed.tolist(), truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(X_val_preprocessed.tolist(), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': y_train_combined.tolist()\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    'input_ids': val_encodings['input_ids'],\n",
        "    'attention_mask': val_encodings['attention_mask'],\n",
        "    'labels': y_val.tolist()\n",
        "})\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english', num_labels=len(label_encoder.classes_),\n",
        "                                                             ignore_mismatched_sizes=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csGCNjBVeNtY",
        "outputId": "b0e21d55-46c9-487c-8692-4f53ddaa41fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# F1 Metric and Training Arguments\n",
        "# -------------------------------\n",
        "# Load the F1 metric and specify macro averaging\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "# Define the compute_metrics function to maximize F1 macro average\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # Compute the F1 macro average\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "    return {\"f1\": f1[\"f1\"]}\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",  # Use F1 score as the metric for saving the best model\n",
        "    greater_is_better=True       # Ensure higher F1 is considered better\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTk9ydkFebCH",
        "outputId": "3a074e6f-f923-4672-8a70-35c6658fa4c7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "collapsed": true,
        "id": "E5SAOkYSegPS",
        "outputId": "603d0c1b-f480-4a2b-84c8-dfe86d44afc3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8136' max='20340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8136/20340 57:56 < 1:26:55, 2.34 it/s, Epoch 8/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.030300</td>\n",
              "      <td>0.297009</td>\n",
              "      <td>0.726214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.084200</td>\n",
              "      <td>0.280447</td>\n",
              "      <td>0.726295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.018900</td>\n",
              "      <td>0.359803</td>\n",
              "      <td>0.856389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.355132</td>\n",
              "      <td>0.842547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.404481</td>\n",
              "      <td>0.863704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.426576</td>\n",
              "      <td>0.846251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.461790</td>\n",
              "      <td>0.856440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.468527</td>\n",
              "      <td>0.848298</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8136, training_loss=0.05991286058624762, metrics={'train_runtime': 3476.4838, 'train_samples_per_second': 46.771, 'train_steps_per_second': 5.851, 'total_flos': 8616908795904000.0, 'train_loss': 0.05991286058624762, 'epoch': 8.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate DistilBERT embeddings for ensemble\n",
        "model.eval()\n",
        "# Ensure model is on the appropriate device (e.g., CUDA if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def get_distilbert_embeddings(encodings, model):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(encodings['input_ids']), 8)):\n",
        "        # Move input tensors to the same device as the model\n",
        "        batch_input_ids = torch.tensor(encodings['input_ids'][i:i+8]).to(device)\n",
        "        batch_attention_mask = torch.tensor(encodings['attention_mask'][i:i+8]).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.distilbert(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "X_train_distilbert = get_distilbert_embeddings(train_encodings, model)\n",
        "X_val_distilbert = get_distilbert_embeddings(val_encodings, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx8z0iBWekVm",
        "outputId": "a1c681c6-18e3-4688-a5fe-4aea72ee70fe"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1017/1017 [02:01<00:00,  8.38it/s]\n",
            "100%|██████████| 128/128 [00:15<00:00,  8.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Combine Features and Ensemble\n",
        "# -------------------------------\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce DistilBERT embeddings to 100 dimensions using PCA\n",
        "pca = PCA(n_components=100, random_state=42)\n",
        "X_train_distilbert_reduced = pca.fit_transform(X_train_distilbert)\n",
        "X_val_distilbert_reduced = pca.transform(X_val_distilbert)\n",
        "\n",
        "# Compute cosine similarity between Word2Vec and reduced DistilBERT embeddings\n",
        "train_cosine_similarity = np.array([\n",
        "    cosine_similarity([wv], [db])[0][0]\n",
        "    for wv, db in zip(X_train_word2vec, X_train_distilbert_reduced)\n",
        "])\n",
        "val_cosine_similarity = np.array([\n",
        "    cosine_similarity([wv], [db])[0][0]\n",
        "    for wv, db in zip(X_val_word2vec, X_val_distilbert_reduced)\n",
        "])\n",
        "\n",
        "# Combine features for the MLP Classifier\n",
        "X_train_combined = np.hstack([X_train_word2vec, X_train_tfidf, X_train_distilbert_reduced, train_cosine_similarity.reshape(-1, 1)])\n",
        "X_val_combined = np.hstack([X_val_word2vec, X_val_tfidf, X_val_distilbert_reduced, val_cosine_similarity.reshape(-1, 1)])\n",
        "\n",
        "# Train MLPClassifier\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=500, random_state=42)\n",
        "mlp_classifier.fit(X_train_combined, y_train_combined)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_val_pred = mlp_classifier.predict(X_val_combined)\n",
        "print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an8jVdoLekPP",
        "outputId": "3b72f3a6-c4bd-4b64-a525-04319c6edce9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                precision    recall  f1-score   support\n",
            "\n",
            "                     allergens       0.95      0.98      0.96       371\n",
            "                    biological       1.00      0.99      0.99       348\n",
            "                      chemical       0.87      0.93      0.90        57\n",
            "food additives and flavourings       0.67      0.40      0.50         5\n",
            "                foreign bodies       0.99      0.98      0.99       112\n",
            "                         fraud       0.74      0.72      0.73        74\n",
            "                     migration       1.00      1.00      1.00         1\n",
            "          organoleptic aspects       0.90      0.82      0.86        11\n",
            "                  other hazard       0.75      0.67      0.71        27\n",
            "              packaging defect       0.90      0.82      0.86        11\n",
            "\n",
            "                      accuracy                           0.94      1017\n",
            "                     macro avg       0.88      0.83      0.85      1017\n",
            "                  weighted avg       0.94      0.94      0.94      1017\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7E_Xb3aKvMOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOad8+BI+YUtOVjVdYwS20Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
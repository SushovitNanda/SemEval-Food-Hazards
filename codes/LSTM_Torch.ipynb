{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv('incidents_labelled.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDDGhUGyhJrp",
        "outputId": "832c8945-2058-4cae-e16c-7b6cde16922e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply text preprocessing to the title column\n",
        "train_data['cleaned_title'] = train_data['title'].apply(preprocess_text)\n",
        "\n",
        "# Tokenize the cleaned_title column\n",
        "tokenizer = Tokenizer(num_words=5000)  # Limit vocabulary to top 5000 words\n",
        "tokenizer.fit_on_texts(train_data['cleaned_title'].values)\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "X = tokenizer.texts_to_sequences(train_data['cleaned_title'].values)\n",
        "\n",
        "# Padding sequences to ensure uniform input length\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "\n",
        "# Converting hazard-category to binary format using LabelBinarizer (for multi-class classification)\n",
        "lb_hazard = LabelBinarizer()\n",
        "y_hazard = lb_hazard.fit_transform(train_data['hazard-category'])\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train_hazard, y_val_hazard = train_test_split(X, y_hazard, test_size=0.2, random_state=42, stratify=y_hazard)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=1, bidirectional=False, dropout=0.5):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        hidden = lstm_out[:, -1, :]  # Take the last hidden state\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "# Set parameters\n",
        "vocab_size = 5000  # Should match tokenizer's num_words\n",
        "embedding_dim = 128\n",
        "hidden_dim = 64\n",
        "output_dim = y_hazard.shape[1]  # Number of classes\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "patience = 5  # Early stopping patience\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n",
        "criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-label classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Prepare data loaders\n",
        "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train_hazard, dtype=torch.float))\n",
        "val_data = TensorDataset(torch.tensor(X_val, dtype=torch.long), torch.tensor(y_val_hazard, dtype=torch.float))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Early stopping variables\n",
        "best_val_loss = np.inf\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Check early stopping condition\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pt')  # Save the best model\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve == patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "# Final evaluation with classification report\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Classification report\n",
        "print(\"Final Classification Report:\")\n",
        "print(classification_report(np.array(all_labels), np.array(all_preds), target_names=lb_hazard.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVv2C35VgahL",
        "outputId": "06449ffb-9c4a-485e-defb-b8c3e2ea0dc8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Training Loss: 0.2927\n",
            "Validation Loss: 0.1896, Validation Accuracy: 0.4302\n",
            "Epoch 2/50, Training Loss: 0.1844\n",
            "Validation Loss: 0.1590, Validation Accuracy: 0.5322\n",
            "Epoch 3/50, Training Loss: 0.1647\n",
            "Validation Loss: 0.1497, Validation Accuracy: 0.5781\n",
            "Epoch 4/50, Training Loss: 0.1530\n",
            "Validation Loss: 0.1454, Validation Accuracy: 0.6032\n",
            "Epoch 5/50, Training Loss: 0.1450\n",
            "Validation Loss: 0.1402, Validation Accuracy: 0.6048\n",
            "Epoch 6/50, Training Loss: 0.1378\n",
            "Validation Loss: 0.1382, Validation Accuracy: 0.6199\n",
            "Epoch 7/50, Training Loss: 0.1321\n",
            "Validation Loss: 0.1360, Validation Accuracy: 0.6316\n",
            "Epoch 8/50, Training Loss: 0.1260\n",
            "Validation Loss: 0.1345, Validation Accuracy: 0.6383\n",
            "Epoch 9/50, Training Loss: 0.1212\n",
            "Validation Loss: 0.1326, Validation Accuracy: 0.6516\n",
            "Epoch 10/50, Training Loss: 0.1165\n",
            "Validation Loss: 0.1317, Validation Accuracy: 0.6583\n",
            "Epoch 11/50, Training Loss: 0.1128\n",
            "Validation Loss: 0.1349, Validation Accuracy: 0.6717\n",
            "Epoch 12/50, Training Loss: 0.1084\n",
            "Validation Loss: 0.1379, Validation Accuracy: 0.6792\n",
            "Epoch 13/50, Training Loss: 0.1051\n",
            "Validation Loss: 0.1414, Validation Accuracy: 0.6859\n",
            "Epoch 14/50, Training Loss: 0.0994\n",
            "Validation Loss: 0.1371, Validation Accuracy: 0.6934\n",
            "Epoch 15/50, Training Loss: 0.0961\n",
            "Validation Loss: 0.1355, Validation Accuracy: 0.6884\n",
            "Early stopping triggered\n",
            "Final Classification Report:\n",
            "                                precision    recall  f1-score   support\n",
            "\n",
            "                     allergens       0.85      0.86      0.85       391\n",
            "                    biological       0.87      0.81      0.84       404\n",
            "                      chemical       0.83      0.19      0.31       100\n",
            "food additives and flavourings       0.00      0.00      0.00         5\n",
            "                foreign bodies       0.87      0.51      0.64       154\n",
            "                         fraud       0.81      0.37      0.50        82\n",
            "                     migration       0.00      0.00      0.00         3\n",
            "          organoleptic aspects       0.00      0.00      0.00        13\n",
            "                  other hazard       0.00      0.00      0.00        29\n",
            "              packaging defect       0.00      0.00      0.00        16\n",
            "\n",
            "                     micro avg       0.86      0.66      0.75      1197\n",
            "                     macro avg       0.42      0.27      0.31      1197\n",
            "                  weighted avg       0.81      0.66      0.71      1197\n",
            "                   samples avg       0.66      0.66      0.66      1197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vcBOVDwjgb3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}